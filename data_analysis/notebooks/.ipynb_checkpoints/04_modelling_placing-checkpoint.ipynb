{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(\"../data/03_modelling/dataset_place.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(path,index_col=0)\n",
    "\n",
    "# open a file, where you stored the pickled data\n",
    "file = open('../data/03_modelling/dict.placing_segments', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "reaching_seg = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##### Predicting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = dataset['next_pos']    #['next_pos']\n",
    "X = dataset.drop(['time_end','next_pos'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate model with Kfold stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling step Test differents algorithms \n",
    "random_state = 2\n",
    "classifiers = []\n",
    "classifiers.append(SVC(random_state=random_state))\n",
    "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(MLPClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state = random_state))\n",
    "classifiers.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "cv_results = []\n",
    "for classifier in classifiers :\n",
    "    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n",
    "\n",
    "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g = g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExtraTrees \n",
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsExtC.fit(X_train,y_train)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC Parameters tunning \n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train,y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC = GradientBoostingClassifier()\n",
    "\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [40,50,60],\n",
    "              'learning_rate': [0.9,0.8,0.7],\n",
    "              'max_depth': [0.5,1,2],\n",
    "              'min_samples_leaf': [30,50,80],\n",
    "              'max_features': [70,50,30] \n",
    "              }\n",
    "\n",
    "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsGBC.fit(X_train,y_train)\n",
    "\n",
    "GBC_best = gsGBC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsGBC.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "g = plot_learning_curve(gsRFC.best_estimator_,\"RF learning curves\",X_train,y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,y_train,cv=kfold)\n",
    "g = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,y_train,cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n",
    "\n",
    "nrows = 2\n",
    "names_classifiers = [(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n",
    "\n",
    "nclassifier = 0\n",
    "for row in range(nrows):\n",
    "    for col in range(ncols):\n",
    "        name = names_classifiers[nclassifier][0]\n",
    "        classifier = names_classifiers[nclassifier][1]\n",
    "        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n",
    "        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n",
    "        g.set_xlabel(\"Relative importance\",fontsize=12)\n",
    "        g.set_ylabel(\"Features\",fontsize=12)\n",
    "        g.tick_params(labelsize=9)\n",
    "        g.set_title(name + \" feature importance\")\n",
    "        nclassifier += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Survived_RFC = pd.Series(RFC_best.predict(X_test), name=\"RFC\")\n",
    "test_Survived_ExtC = pd.Series(ExtC_best.predict(X_test), name=\"ExtC\")\n",
    "#test_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\n",
    "#test_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\n",
    "test_Survived_GBC = pd.Series(GBC_best.predict(X_test), name=\"GBC\")\n",
    "\n",
    "# Concatenate all classifier results\n",
    "ensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_GBC],axis=1)\n",
    "\n",
    "\n",
    "g= sns.heatmap(ensemble_results.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc',ExtC_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n",
    "\n",
    "votingC = votingC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(votingC.predict(X_test), name=\"next_pos\")\n",
    "\n",
    "#results = pd.concat([IDtest,test_Survived],axis=1)\n",
    "\n",
    "#results.to_csv(\"ensemble_python_voting.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(votingC, X_test, y = y_test, scoring = \"accuracy\", cv = kfold, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
